# Amazon Ratings and Review Analysis

# Amazon Review Analysis

# Introduction: -
The quality of the comments made on internet forums has always suffered due to anonymity of its users. When users’ comments on, for instance, a YouTube video, there is no repercussion for what they say, and the dialog generated is often not helpful. Different websites have tried different methods for extracting more useful comments. Reddit uses an up-vote system, Quora fosters a community that values high quality responses over low quality ones, and Amazon allows for its users to rate the “helpfulness” of reviews left on their products. Amazon’s system, in particular, then allows for the higher rated comments to be displayed at the top of the review forum so that new users can see the top-rated comments in order to help them make their own purchasing decisions.
Even though Amazon’s helpfulness rating system seems to work on the surface level, poor quality comments still seem to be at the top of their review forms. Having poor quality reviews hurts Amazon’s business, as a major reason that people are willing to buy consumer goods on-line without seeing the items themselves, is that they have access to others people’s opinions of the item. For example, a review at the top of an app called “Friday Night at Freddie’s 4” is as follows:
  “I love this game so much but at first I though it was lame but when I go in the game I can't beat the first night because cause I put it to full volume and I can't here the breathing bonnie strike at 4 am Chica at 5 and plus it not lame it's better than fnaf and fnaf 2 plus get this game when u buy fnaf”
This comment, despite being at the top of the forum, is difficult to understand, a run on sentence, and full of spelling errors. The reason for the failure is part of the algorithm for determining the order of the reviews relies on how recently the review has been made. The offending review was the most recent, but it’s helpfulness score was far less than previous reviews. This illustrates the difficult balance that must be struck between showing the highest rated reviews, and showing the newest reviews, to be rated by the community. An ideal system would predict if a review is helpful or not, so that poor quality reviews would not need to be displayed the top.

# Problem Statement: -
The problem being addressed in this project is the poor quality of Amazon reviews at the top of the forum despite the “helpfulness” rating system. The problem arises from the “free pass” given to new reviews to be placed at the top of the forum, for a chance to be rated by the community. The proposed solution to this problem is to use machine learning techniques to design a system that “pre-rates” new reviews on their “helpfulness” before they are given a position at the top of the forum. This way, poor quality reviews will be more unlikely to be shown at the top of the forum, as they do not get the “free pass” because they are new. The proposed system will use a set of Amazon review data to train itself to predict a helpfulness classification (helpful, or not helpful) for new input data.

# Metrics: -
Since our problem is a binary classification problem (helpful or not helpful). We will use the ‘Receiver Operator Characteristic Area Under the Curve’ or roc_auc score to evaluate our model. The curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR). The area under the curve is used to give a score to the model. If the area under the curve is 0.5, then the TPR is equal to the FPR, and the model is doing no better than random guessing. A perfect model would have an AUC of 1.0, meaning it has 100% TPR.

# Analysis: [In1]
Data Exploration – First we will check if we have already downloaded the dataset. If not, we will download it. [In2]
We need to extract the data, and convert the data given in "json" to a dataframe in order to perform our analysis. To save time for later we will pickle the file. [In3]
OK now let’s have a look at the data. [In4]
The helpful score can be explained as follows: a user can either rate the review as 'helpful’ or ‘not helpful’. The dataset records each of these in an array. For our problem we want to classify the email as either ‘helpful’ or not ‘helpful’. For training, this label can be generated by dividing the ‘helpful’ ratings by the total ratings and seeing if it exceeds a certain threshold.
Let’s extract the information we need for this project:
* The 'summary' will be used to generate features using natural language processing.
* The score will be used as a feature in our final model. It will not be included in the benchmark model.
* The 'helpful' rating will be used to generate labels. We will train our model using these training labels. Predict the label using the test features, and measure the success of our model using the test labels. This will be explained more later. 
Other features that could be important would be the summary; however, in the interest of keeping this simple, we will exclude it from the analysis. [In5]
Let’s have a quick look at the stats... [In6]
Exploratory Visualization:
let's have a look at a couple of the outliers and make sure they are valid data. [In7][In8]
This summary looks fine to me. The amount in the helpful_numerator and denominator won't matter as we are going to transform these labels later on.
Benchmark:
As a benchmark for this project we are going to use an out of the box algorithm trained on features generated using NLP techniques. This will be discussed is greater detail in later sections.

# Methodology:
  Data Pre-processing:
Data that has had less than 10 ratings will first be trimmed out of the dataset. There are a significant amount of reviews, and many do not face the scrutiny of people reading them. As many of these reviews could be good, but by chance, do not get read and rated, they will be rated negatively by our algorithm and will cause our model to incorrectly classify future "good" reviews. [In9]
In order to perform our analysis, we wish to determine if a given review text is helpful or not. We need a way to map the existing data to this binary classification. The chosen method is to use a threshold of 'helpful_numerator' divided by 'helpful_denominator'. In other words, the ratio of the people who found the review helpful over the amount of people who rated the review as helpful or unhelpful. If this ratio exceeds a certain threshold value, we can label the training data as 'helpful' = 1, or 'non-helpful' = 0. For this analysis, the threshold is arbitrarily set to 0.5, however, this could be tuned in the future to a higher value in order to better filter the reviews. [In10]
Now let’s do a visualization and a count of the data in order to get a sense of the correlation and distribution. [In11] [In12]
We can see that our dataset in unbalanced, with roughly 7 times as many effective reviews as ineffective reviews. This means that when we are splitting our data later, we will need to use a stratified shuffle so that both categories have proportional representation in training.
We can also see that there is a small correlation between the score and the 'Helpful' rating. It is good that this correlation is small, or else our job would be very easy and we wouldn't need to generate features from the summary at all, we could just look at the score of the review.

  Text Feature Generation for Benchmark model:
Now we can convert the text to lower case so we can run our natural language processing to produce features. We could also remove punctuation. But removing punctuation may seem like it should not be done in the case of this problem, as not having punctuation will make a review harder to understand. This is a trade-off that we must consider. If we include punctuation, it will cause the learning algorithm to behave poorly. Therefore, we would refrain from doing so. [In13]
In order to generate more features, we will use the TF-IDF vectorizer from the sci-kit learn library. We will also use the NLTK library. The pre-processing methods that we are going to employ are as follows:
* Stemming - Stemming means to take off the suffixes of the stemmed word. Therefore, words such as "run" and "running" would both be represented as "run". This allows for our algorithms to more accurate find trends in the "meanings" of sentences (e.g. "the sun is shining" and "the sun shines" will more accurately be associated with the same meaning. It will reduce the total amount of features that we will generate.
* Tokenizing- Splits sentences up into single words. This is needed to generate our TFIDF features.
* Remove Stop Words- This moves words such as "the" "a" and "it" as shown in the English stop words corpus. These will only clutter up our learning algorithm. Admittingly, sentences without these words are hard to understand, and ideally, they should be kept, but it is again a trade-off.
* ngrams- Makes groups of words that are 'n' long. E.g the 2-grams for the sentence "The shaggy dog" are [The, shaggy], [The, dog] and [shaggy, dog]. Weving more than that is computationally expensive for my computer. will stick to 2-grams and 1-grams for now. Having more than that is computationally expensive.
Finally, we will generate TF-IDF scores for each of the stemmed and tokenized words and ngrams. TF-IDF is short for Term Frequency Inverse Document frequency. TF-IDF is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining.
  TF: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:
TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).
IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However, it is known that certain terms, such as "is", "of", and "that", may appear a lot of times but have little importance. Thus, we need to weigh down the frequent terms while scale up the rare ones, by computing the following:
IDF(t) = log_e(Total number of documents / Number of documents with term t in it).
For our vectorizer, we will also set the min_df to 0.001. This is a cheap way of getting rid of spelling mistakes. Words that appear this infrequently are most likely typos and are uninteresting to our algorithm. It will also reduce our feature space, allowing for our algorithm to work faster. [In14]
We have now generated all of the features that we are going to need.

  Benchmark model:
In order to establish a baseline for the project, we will look at all of the out of box models from sklearn.
The following algorithms were investigated:
* Gaussian Naive Bayes (GaussianNB)
* Decision Trees
* Ensemble Methods (Bagging, AdaBoost, Random Forest, Gradient Boosting)
* K-Nearest Neighbours (Neighbours)
* Stochastic Gradient Descent (SGDC)
* Support Vector Machines (SVM)
* Logistic Regression
Taking this documentation into consideration, the following three algorithms have been selected to be investigated, for reasons listed below:
Logistic Regression: Logistic regression, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier.
* Real World Application: Logistic Regression is used frequently for text classification, and is popular in Kaggle competitions as a baseline classifier.
* Strengths of Model: Logistic regression can be updated after is has already been trained, meaning that new reviews can be used to teach the algorithm after it has already been training. 
* Weaknesses of Model: Logistic regression is not as fast as the naive bayes methods, but then again, not many algorithms are.
* Selection Justification: The ability to update itself after its initial training period may prove to be valuable.
Random Forest: A Random Forest is an ensembling algorithm that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. It does not implement boosting on the trees, and instead counts a vote of each individual tree in order to produce the final class label.
* Real World Application: Random Forest has been used in Diabetic Retinopathy classification analyses (5), and is very popular for Kaggle competitions.
* Strengths of Model: The strength of the Random Forest classifier comes from the formation of its trees. Because it is formed from "random" subsets of the data, and the final result is compared to other trees that have also been formed "randomly", the algorithm guards well against "overfitting" from noisy data points that may have more influence on a single decision tree algorithm. The "random" formation of the trees ensures that there is little chance for a strong bias to be present in the data during tree construction. (4) Random Forests also work well with high dimensional data, as in the case of our TFIDF features. (5).
* Weaknesses of Model: A lot of trees are necessary to get stable estimates of variable importance and proximity. This can lead to a large amount of space in memory being needed to store the trees. Additionally, the trees need to be re-trained when new data is being introduced, unlike Naive Bayes. Its training complexity is given as O (M √d n log n) where d is the number of features and M is the number of trees. 
* Selection Justification: Since random forest works well with high dimensional data, as is competitive with other algorithms such as SVM without the high training cost (5) it is selected for our study.
Adaboost: AdaBoost or "adaptive boosting" begins by fitting a "weak" classifier on the original dataset. It then fits additional copies of the classifier on the same dataset and adjusts the weights of incorrectly classified instances such that subsequent classifiers focus more on difficult cases. The adjustment is done using the SAMME-R algorithm (for classification)
* Real World Application: Adaboost has been used in robust real time face detection (6). It is also quite popular for Kaggle competitions.
* Strengths of Model: Because of its boosting property, Adaboost will not suffer from overfitting caused by too many training periods in the boosting algorithm. Theoretically, the algorithm should produce better and better results the more it is trained.
* Weaknesses of Model: Adaboost is slow to train, relative to GaussianNB (actual training complexity could not be found). 
* Selection Justification: It will be interesting to measure the performance of the boosting method of Adaboost vs the vote method for the RF algorithm, as both use decision trees and differ only in the method in which the tree are ensembled together for the final classification.
Now we will shuffle and split the data into 80% training and 20% testing. [In15]
The benefit to splitting the data into testing and training sets is that this allows simulated evaluation of how well the model is performing before using it in the real world to make predictions. If there was no testing set, there would be no way of realistically evaluating the model as you would only be able to measure its performance on data to which it's already been exposed. This will result in a false sense of confidence in how well the model performs. [In16]
Setup: [In17]
We will now test our classifiers using the functions above. All of the classifiers from sklearn will be tested, but only the three top choices discussed will have the results recorded in a table. We will train and test a bunch of the classifiers on three different training sizes in order to find which one will be our benchmark. [In17]
It seems that the Logistic Regression classifier did the best in our benchmark test. It's interesting to compare the speed of the Logistic Regression to the other algorithms. It is quite a bit faster than all of the algorithms, and more accurate than each of them for a test size of 38,000. In order to make sure we are covering all of our bases; we should also do a visualization. [In18]
As expected, the Logistic Regression is the best algorithm in terms of accuracy for all test sizes. It's final score for the area under the ROC curve was 0.7448 and a sample size of ~38,000. In addition, it is the fastest. The training speed and prediction speed were 0.538s and 0.0137s respectively for a sample size of 38,000. As our system needs to consider the trade-off between accuracy and speed, the Logistic Regression algorithm represents the ideal model for our benchmark. Surprisingly, the Random Forest algorithm did quite poorly in our tests. It was both slower and less accurate than the Naive Bayes algorithm.


# Results

Model Evaluation and Validation
There are several variables that we can examine in order to determine the robustness of our solution. The average ROC_AUC score for 100 different random states is equal to 0.7795, the same as our optimum solution. In this regard, our solution can be considered robust.

Justification
Our goal in the beginning of this project was to design a system to automatically classify new Reviews made on amazon products as 'helpful' or 'non-helpful'. Our optimum model will correctly do this 77.8% of the time. This means that 22.2% of the time, our system will not work. As someone managing this project, I would not be happy with this result and would seek to improve it.
Our algorithm is able to make predictions on the order of 0.01s per prediction. As this was done on my 8 year old laptop it is reasonable to assume that this could be improved significantly with better hardware. Further analysis that takes into account how many reviews are expected to be made would need to take place before I could determine if this is considered reasonable for this project.

# Improvements
*Grid Search, Cross-Validaiton
In order to improve the accuracy of our model, the following actions could be taken:
* Perform more feature engineering, such as proper spell checking for the reviews. This would result in a model that potentially has less features, as certain spelling errors would have been corrected/eliminated.
* Explore the effect of not doing pre-processing on the reviews. Reviews with poor grammar, and punctuation and improper word endings are more difficult to understand and would possibly lead to people rating them as less helpful. During our project, we did this pre-processing in order to make the algorithms work better; however, maybe feature reduction should have been done in a different way to order to preserve the "incorrect" information.

# References
1. Inferring networks of substitutable and complementary products. J. McAuley, R. Pandey, J. Leskovec Knowledge Discovery and Data Mining, 2015.
2. Image-based recommendations on styles and substitutes J. McAuley, C. Targett, J. Shi, A. van den Hengel SIGIR, 2015.
3. HUI BWU Y. Anti-spam model based on semi-Naive Bayesian classification model. Journal of Computer Applications. 2009;29(3):903-904.
4. Liaw A. Weiner M. Classification and Regression by randomForest. R News. 2002; Vol 2(2):18-22.
5. Casanova R, Saldana S, Chew EY, Danis RP, Greven CM, et al. (2014) Application of Random Forests Methods to Diabetic Retinopathy Classification Analyses. PLoS ONE 9(6): e98587. doi: 10.1371/journal.pone.0098587
6. Jones M. Viola A. Robust Real-Time Face Detection. International Journal of Computer Vision. 2004. pg 137–154.
7. http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/
